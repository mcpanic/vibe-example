# üß† My Active 12 Problems (Context)


## 1. Project: Turning human‚ÄìAI interaction telemetry into organizational insight
* The Goal: Help organizations see themselves through how work actually happens with AI in the loop.
* The Bottleneck: Telemetry is either too raw (logs, counts) or too abstract (dashboards without actionability).
* Looking For: Intermediate representations between low-level events and high-level strategy, metrics managers can trust without oversimplifying human work, and design principles for reflective dashboards that provoke better decisions.

## 2. Project: Measuring productivity and skills through turn-by-turn interaction with AI coding agents
* The Goal: Using telemetry integrated into software engineers‚Äô IDE, trace and extract insights into productivivty and skill
* The Bottleneck: tracking meaningful insights beyond surface-level usage statistics
* Looking for: ways to track, compute, or cmodel meaningful insights from telemetry data

## 3. Project: Designing a flagship talk on successful AI adoption & human‚ÄìAI collaboration
* The Goal: A coherent, opinionated narrative explaining why AI adoption fails, what successful human‚ÄìAI collaboration actually looks like, and how organizations should intervene (tools, culture, incentives).
* The Bottleneck: The space is saturated with hype, shallow case studies, and tool-centric narratives that ignore human skill, task structure, and organizational context.
* Looking For: A reusable conceptual framework (tasks √ó skills √ó risk √ó feedback loops), sharp non-obvious examples from real deployments, and a talk structure that scales from keynote ‚Üí exec briefing ‚Üí classroom lecture.

## 4. Project: A task-centric theory of AI impact on human skill development
* The Goal: Explain when AI use leads to skill growth, skill atrophy, or skill reconfiguration‚Äîat the level of tasks rather than tools.
* The Bottleneck: Existing metrics focus on outcomes or usage frequency, not the nature of cognitive work being offloaded, transformed, or amplified.
* Looking For: A task-level taxonomy linking AI assistance to learning mechanisms, longitudinal signals distinguishing productive dependence from deskilling, and empirical hooks connecting HCI theory to real-world telemetry.

## 5. Project: Evaluating LLMs and AI systems based on human requirements rather than benchmarks
* The Goal: Move beyond static benchmarks toward evaluation frameworks grounded in human goals, constraints, and failure modes.
* The Bottleneck: Benchmarks reward model-centric optimization and obscure blind spots that matter in practice (trust, overreliance, miscalibration).
* Looking For: Requirement-driven or scenario-based evaluation methods, ways to surface unknown unknowns in real usage, and evaluation artifacts legible to practitioners as well as researchers.

## 6. Project: Building an AI-augmented personal knowledge system that actually compounds insight
* The Goal: A PKM system where AI does not just summarize or retrieve, but actively challenges, recombines, and stress-tests ideas across projects.
* The Bottleneck: Most AI‚ÄìPKM workflows optimize for ingestion, not synthesis or long-term intellectual leverage.
* Looking For: Agent workflows that routinely run Feynman checks against active problems, mechanisms to connect new readings to a small set of core research questions, and signals for identifying underdeveloped, redundant, or high-leverage ideas.

## 7. Project: Understanding how generative UI changes the role of HCI and frontend engineering
* The Goal: Articulate what new work emerges when UI is cheap, ephemeral, and automatically generated.
* The Bottleneck: Narratives oscillate between ‚Äúfrontend is dead‚Äù and incremental tooling improvements, missing deeper shifts in design, evaluation, and personalization.
* Looking For: Research directions enabled by one-off or intent-driven UIs, new roles for HCI researchers as system designers and evaluators, and concrete examples of qualitatively new interaction paradigms.

